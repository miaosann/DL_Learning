# 优化神经网络

### 1、优化算法

- ##### 梯度下降

  最简单的优化算法是梯度下降（gradient descent (GD)），对m个样本来说，每次迭代都需要计算所有m个样本的值，完成前向传播、后向传播和梯度更新, 又叫Batch Gradient Descent。

  > 参数更新的方式即为以前的方式。

- ##### Momentum

  因为mini-batch算法是每个mini-batch迭代一次，梯度下降时会有一定的震荡，利用Momentum算法可以减少震荡。Momentum算法计算速度v时不只是参考当前的速度和加速度，而是参考了之前的综合速度和当前的加速度，可以认为是之前几次的速度与此次的加速度综合出来的一个速度。 

  `v["dW" + str(l+1)] = beta*v["dW" + str(l+1)] + (1-beta)*grads["dW" + str(l+1)]`

  此算法为我们算出了一个速度，使得mini-batch不那么震荡。

  - β 过大，相当于加速度不明显，则曲线过于平缓
  - β 太小，接近于标准梯度下降算法，起不到平缓震荡的作用

- β 通常选则[0.8, 0.999] ,一般选择0.9就可以了

  - 也可以尝试几个不同的ββ，看看哪种对于缩小损失函数 J 的作用更好

  > 为参数更新增加了一个v，更新参数来进行减小震荡

- ##### Amda

  dam算法是训练神经网络的最有效的优化算法之一，它综合了RMSProp算法和Momentum算法。

  算法步骤为：

  ​								![amda](https://github.com/miaosann/DL_Learning/blob/master/lesson2_week2/assignment2/images/amda.PNG)

  > 运用之前的v，实现了一种更好的更新参数方式。

### 2、分批次寻最优解

- ##### Mini-Batch梯度下降

  对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，如有500万或5000万的训练数据，处理速度就会比较慢。

  但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为Mini-batch。

  - mini-batch 算法分为两个阶段：shuffle 和 partition
  - mini-batch 大小应该取2的幂次，比如：16，32，64，128

  > 提供一种大数据量下分批梯度下降的思想

### 3、各种方法的尝试

- ##### Mini-batch Gradient descent

- ##### Mini-batch Momentum

- ##### Mini-batch Adam

> Adam算法明显比其它两种算法效果要好，如果你循环更多的代数（走完一次全量为一代）会发现三种算法表现都不错，不过Adam收敛的更快



