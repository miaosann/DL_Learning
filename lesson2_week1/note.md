# 优化神经网络

### 1、初始化参数

- ##### 初始化为0

  参数W和b均初始化为0，效果很不好，准确率一半，还不如猜呢，并且随着迭代次数增加，cost并没有任何变化。零值初始化无法打破对称性，导致每个神经元学习同样的内容，相当于只有一层，和线性逻辑回归没什么区别。

  > 所以我们尝试随机大值初始化

- ##### 随机初始化（较大数）

  参数W初始化为随机数*10，效果还不错，用大值随机初始化时开始cost很大，逐步缩小，但下降缓慢，不好的初始化会导致梯度消失或梯度爆炸，也拖慢了算法优化的速度。

  > 看来小值初始化会表现的更好，但是小值是多小呢？

- ##### He初始化（较小数）

  He 初始化W1：`sqrt(2./layers_dims[1-1])` ，这种方法在不多的循环下，分类效果就已经很好了。

  > 所以我们推荐以后使用这种初始化的方法。

### 2、防止过拟合

- ##### 没有正则化的模型

  当我们将图形画出来后，会发现训练集出现了不同程度的过拟合，适应噪声点，这并不是我们想要的结果。

  > 所以我们尝试用以下两种方式来改进过拟合问题

- ##### L2正则化的模型

  我们在原来的基础上为其损失函数增加一项`(1./m*lambd/2)*(np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))`，同时在反向传播时也对应（如对layer3进行举例）`dW3 = 1./m * np.dot(dZ3, A2.T) + lambd/m * W3`，根据结果可以看出，正则化成功解决了过拟合现象。

- ##### Dropout的模型

  我们随机为神经网络中每层进行失活（如对layer3进行举例）

  ```python
  keep_prob = 0.8  # 设置神经元保留概率
  d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob
  a3 = np.multiply(a3, d3)
  a3 /= keep_prob
  ```

  我们之所以在最后进行`a3 /= keep_prob`，是因为现在每一层都有20%的神经元失活，下一层使用a3进行计算，为了不影响Z4的期望值，所以这样。

### 3、梯度检验

- 就这样吧，不太理解这样的用处。

