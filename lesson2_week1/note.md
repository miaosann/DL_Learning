# 优化神经网络

### 1、初始化参数

- ##### 初始化为0

  参数W和b均初始化为0，效果很不好，准确率一半，还不如猜呢，并且随着迭代次数增加，cost并没有任何变化。零值初始化无法打破对称性，导致每个神经元学习同样的内容，相当于只有一层，和线性逻辑回归没什么区别。

  > 所以我们尝试随机大值初始化

- ##### 随机初始化（较大数）

  参数W初始化为随机数*10，效果还不错，用大值随机初始化时开始cost很大，逐步缩小，但下降缓慢，不好的初始化会导致梯度消失或梯度爆炸，也拖慢了算法优化的速度。

  > 看来小值初始化会表现的更好，但是小值是多小呢？

- ##### He初始化（较小数）

  He 初始化W1：`sqrt(2./layers_dims[1-1])` ，这种方法在不多的循环下，分类效果就已经很好了。

  > 所以我们推荐以后使用这种初始化的方法。

